---
title: "Case3 Galferie lafayette"
author: "Raiyan Puntel and Mahmoud Alkheja"
date: "2023-04-11"
output:
  html_document:
    toc: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls()) #Clean the entire environment
cat("\014") # clean console
```

### Loading libraries and Reading the data set:

```{r echo=TRUE, message=FALSE}
## set library
library(lavaan) 
library(semPlot) #for visualization
library(knitr)
library(dplyr)
library(lavaanPlot)
library(lm.beta)
library(rcompanion)   #Histogram and Normal Curve
library(nortest)      #Kolmogorov-Smirnov-Test
library(corrplot)     #correlation matrix plot
library(olsrr)        #VIF and Tolerance Values
library(pastecs)
library(REdaS)        #Bartelett's Test
library(psych)        # principal axis factoring 
library(naniar)       # for missing values analysis
library(RColorBrewer)
library(ggcorrplot)
```

```{r}
setwd("/Users/mahmoudalkheja/Desktop/Advanced Data Driven Decision Making/Case Study III-20230420")
myData <- read.csv("Case Study III_Structural Equation Modeling.csv")
explanation <- read.csv("Variables and Labels_Galeries Lafayette.csv")

```

```{r}
myData =data.frame(sapply(myData,function(x) ifelse((x==999),NA,as.numeric(x))))

```

Change 999 in the Data-set to NA's:

# Exploratory factor analyses

**Conducting the confirmatory factor analyses run an exploratory factor analyses in R to get an initial idea by which dimensions customers perceive Gallerias Lafayette. In the file all 22 image items are proposed to measure different constructs or perceptual dimensions**

### Explore the data

```{r}
head(myData)
summary(myData)
dim(myData)

```

Our dataset consists of 553 observations and 45 features, but it exhibits missing values in certain portions.
Additionally, all questions (Image 1 to 22) in the dataset have been scaled on a 7-point scale.

```{r}
gg_miss_var(myData)
```

For **exploratory factor analysis**: we only consider variables image1 to image22, and we will use listwise deletion to handle missing data before starting.

```{r}
image <- myData[,c(1:22)]
miss_var_summary(image)
image <- na.omit(image)
dim(image)
```

385 observation after removing the missing values.

#### **Normality assumption**

```{r}
# histograms 
par(mfrow = c(3, 3))
for (i in colnames(image)) {
  plotNormalHistogram(image[,i], main = paste("Frequency Distribution of", i))
}
```

```{r}
lillie.test(image$Im1)   # Kolmogorov-Smirnov-Test for normality 
lillie.test(image$Im22)   # Kolmogorov-Smirnov-Test for normality 
```

Upon examining the histograms of the data, it appears that the normality assumption is not met.
Specifically, the Kolmogorov-Smirnov test for normality on samples (Im1, Im22) yielded a small p-value, indicating that the null hypothesis that the sample comes from a normal distribution can be rejected.

### Correlation matrix:

```{r}
Matrix = cor(image, use="complete.obs") #We create a matrix.
ggcorrplot(round(as.matrix(Matrix), 2),
           method = "square", 
           type = "lower", 
           show.diag = FALSE,
           lab = TRUE, lab_col = "black", hc.order = T, lab_size = 2)
```

Given that most of the correlation coefficients among the variables exceed 0.3, it appears that there is a significant degree of correlation between the variables.
As a result, factor analysis can be an appropriate method to extract underlying factors from these correlated variables.

### **Check adequacy of correlation matrix.**

```{r}
KMOTEST=KMOS(image)
KMOTEST

sort(KMOTEST$MSA)
```

KMO ( Kaiser-Meyer-Olkin ) test gives us a KMO - criterion of 0.88 which is good we need more than 0.6 for a good factor analysis.
We see no gap between the variables they are all very close in KMO.

```{r}
bart_spher(image)
```

In the Bartlett's Test of Sphericity the small p_value indicate strong evidence against the null hypothesis that CM equals to identity matrix in other word indicating that the variables in the data set are significantly correlated

### **Principal axes factoring**

```{r}
# Run factor analysis with no rotation
# ?fa  # details on the function

fa_0 <- fa(image, 
           nfactors = ncol(image), 
           rotate = "none")

# Look at communalities
sort(fa_0$communalities)
```

```{r}
total_var_explained_paf <- data.frame(
  Factor_n = as.factor(1:length(fa_0$e.values)), 
  Eigenvalue = fa_0$e.values,
  Variance = fa_0$e.values/(ncol(image))*100,
  Cum_var = cumsum(fa_0$e.values/ncol(image))
  )
total_var_explained_paf
```

The first factor explains 40.80% of the total variance, indicating a relatively strong ability to capture the underlying structure of the data.
Additionally, the first six factors have Eigenvalues greater than 1, collectively accounting for 76.64% of the total variance.
However, the seventh and the eighth factors have an Eigenvalue less than 1.
BY adding them the explained total variance will be 83.54%.

```{r}
# Scree plot
ggplot(total_var_explained_paf, aes(x = Factor_n, y = Eigenvalue, group = 1)) + 
  geom_point() + geom_line() +
  xlab("Number of factors") +
  ylab("Initial eigenvalue") +
  labs( title = "Scree Plot") +
  geom_hline(yintercept= 1, linetype="dashed", color = "red")
```

According to the Kaiser criterion, we should extract factors with eigenvalues larger than 1, which would suggest retaining six factors.
However, it is worth noting that the eigenvalue for the 7th factor is close to 1.

**Factor rotation and factor interpretation.**

```{r}
fa_paf_6f <- fa(
  image,
  fm = "pa",              # principal axis factoring
  rotate = "varimax",     # varimax rotation
  nfactors = 6            # 6 factors
  )
```

```{r}
communalities_6f <- data.frame(sort(fa_paf_6f$communality))
communalities_6f
```

```{r}
print(fa_paf_6f$loadings, cutoff=0.3, sort = TRUE)
```

Based on the loadings of the variables and their relationship to the factors, it appears that the 6-factor model does not fit the data well.
Many variables seem to have loadings on multiple factors such as lm 17 and lm9, which can indicate a lack of discriminant validity.
Therefore, it may be necessary to explore other solutions.

One possible solution is to consider a 7-factor model, as suggested by the Kaiser criterion.
This would involve extracting an additional factor and re-analyzing the data to assess the fit of the model.

```{r}
fa_paf_7f <- fa(
  image,
  fm = "pa",              # principal axis factoring
  rotate = "varimax",     # varimax rotation
  nfactors = 7            # 7 factors
  )
```

```{r}
communalities_7f <- data.frame(sort(fa_paf_7f$communality),
                               sort(fa_paf_6f$communality))
communalities_7f
```

```{r}
print(fa_paf_7f$loadings, cutoff=0.3, sort=TRUE)
```

After implementing a 7-factor model, it appears that the results have improved.
However, there are still some variables that have loadings on multiple factors such as lm 8,lm 16 and lm 19, which suggests a lack of discriminant validity.
Additionally, there are some variables with low loading values such as lm15 and lm 11, indicating that they may not be contributing much to the underlying factors.
Let's try to interpret the results :

1)  What do GLB represent from your point of view?
    Large Assortment

2)  What do GLB represent from your point of view?
    Assortment Variety

3)  What do GLB represent from your point of view?
    Artistic Decoration of Sales Area

4)  What do GLB represent from your point of view?
    Creative Decoration of Sales Area

5)  What do GLB represent from your point of view?
    Appealing Arrangement of Shop Windows

6)  What do GLB represent from your point of view?
    France

7)  What do GLB represent from your point of view?
    French Savoir-vivre

8)  What do GLB represent from your point of view?
    Expertise in French Traditional Cuisine

9)  What do GLB represent from your point of view?
    French Fashion

10) What do GLB represent from your point of view?
    Gourmet Food

11) What do GLB represent from your point of view?
    High-quality Cosmetics

12) What do GLB represent from your point of view?
    Luxury brands

13) What do GLB represent from your point of view?
    Up to date Designer Brands

14) What do GLB represent from your point of view?
    Gourmet specialties

15) What do GLB represent from your point of view?
    Professional Selection of Brands

16) What do GLB represent from your point of view?
    Professional Appearance Towards Customers

17) What do GLB represent from your point of view?
    Are Trendy

18) What do GLB represent from your point of view?
    Are Hip

19) What do GLB represent from your point of view?
    Professional Organization

20) What do GLB represent from your point of view?
    Relaxing Shopping

21) What do GLB represent from your point of view?
    A Great Place to Stroll

22) What do GLB represent from your point of view?
    Intimate Shop Atmosphere

**Factors interpretation**

-   PA5 -\> 1,2,15,16,19 --\> Variety ( Im15 has a low loading seem to be not relevant and more about professional,lm16 and lm19 have loadings on PA1 as well and it is about professiona)

-   PA1 --\> 3,4,5,16,19 --\> Decoration

-   PA3 --\> 20,21,22 --\> Atmosphere or Ambiance

-   PA2 --\> 8,10,14 --\> Food or Cuisine ( Im 8 has as well a loading on factor 7 )

-   PA4 --\> 9,11,12,13 --\> Brand (Im11 has a low loading )

-   PA7 --\>6-7-8-9 --\>Related to France (lm8 and lm9 have loadings on other factors)

-   PA6 --\> 17 , 18 --\> Fashion or mode.

```{r}
fa_paf_8f <- fa(
  image,
  fm = "pa",              # principal axis factoring
  rotate = "varimax",     # varimax rotation
  nfactors = 8            # 8 factors
  )
```

```{r}
communalities_8f <- data.frame(sort(fa_paf_8f$communality),
                               sort(fa_paf_7f$communality),
                               sort(fa_paf_6f$communality))
communalities_8f
```

```{r}
print(fa_paf_8f$loadings, cutoff=0.3, sort=TRUE)
```

Based on our initial analysis, an 8-factor solution appears to be a good fit for our data.
However, to further refine our results, we will re-do the analysis after removing variables 8,9 and 15 since they have low loading on two factors.

```{r}
fa_paf_8f_n <- fa(
  image[,-c(8,9,15)],
  fm = "pa",              # principal axis factoring
  rotate = "varimax",     # varimax rotation
  nfactors = 8            # 8 factors
  )
```

```{r}
communalities_8f_n <- data.frame(sort(fa_paf_8f_n$communality))
                            
communalities_8f_n
```

```{r}
print(fa_paf_8f_n$loadings, cutoff=0.3, sort=TRUE)
```

```{r}
fa.diagram(fa_paf_8f_n)
```

**Factors interpretation**

-   PA1 --\> 3,4,5 --\> Decoration

-   PA3 --\> 20,21,22 --\> Atmosphere or Ambiance

-   PA4 --\> 11,12,13 --\> Brand

-   PA5 -\> 1,2 --\> Variety

-   PA2 --\> 10,14 --\> Food or Cuisine

-   PA7 --\> 6-7 --\> Related to France

-   PA6 -\> 17-18 -\>Fashion or Mode

-   PA8 -\> 16-19 -\>Professionalism

### PCA

```{r}
# run factor analysis
fa_pca <- principal(
  image, 
  rotate="none", 
  scores=TRUE)

# data frame with eigenvalues
pca <- data.frame(
  Factor_n =  as.factor(1:length(fa_pca$values)), 
  Eigenvalue_PCA = fa_pca$values,
  Eigenvalue_PAF = fa_0 $e.values
  )
pca
```

The eigenvalues obtained from performing PAF and PCA factor analyses exhibit a high degree of similarity and often yield identical results.
This is a common occurrence in these types of analyses.

```{r}
fa_pca_7f <- principal(
  nfactors = 7,
  image, 
  rotate="varimax", 
  scores=TRUE           # If TRUE, find component scores
  )

pca_communalities_7f <- data.frame(fa_pca_7f$communality,
                                   fa_paf_7f$communality)
pca_communalities_7f
```

As expected, PCA often yields higher communalities estimates than PAF.

```{r}
print(fa_pca_7f$loadings, cutoff=0.3, sort=TRUE)
```

Same results from PAF .
After implementing a 7-factor model, there are still some variables that have loadings on multiple factors such as lm 8,lm 16 and lm 19, thus we will try with 8 factors :

```{r}
fa_pca_8f <- principal(
  nfactors = 8,
  image, 
  rotate="varimax", 
  scores=TRUE           # If TRUE, find component scores
  )

pca_communalities_8f <- data.frame(fa_pca_8f$communality,
                                   fa_paf_8f$communality)
pca_communalities_8f
```

```{r}
print(fa_pca_8f$loadings, cutoff=0.3, sort=TRUE)
```

8-factor solution appears to be a good fit for our data.
However, to further refine our results, we will re-do the analysis after removing variables 8,9 and 15 since they have low loading on two factors.

```{r}

fa_pca_8f_n <- principal(
  nfactors = 8,
  image[,-c(8,9,15)], 
  rotate="varimax", 
  scores=TRUE           # If TRUE, find component scores
  )

pca_communalities_8f_n <- data.frame(fa_pca_8f_n$communality,
                                   fa_paf_8f_n$communality)
pca_communalities_8f_n
```

```{r}
print(fa_pca_8f_n$loadings, cutoff=0.3, sort=TRUE)
```

**Factors interpretation**

1)  What do GLB represent from your point of view?
    Large Assortment

2)  What do GLB represent from your point of view?
    Assortment Variety

3)  What do GLB represent from your point of view?
    Artistic Decoration of Sales Area

4)  What do GLB represent from your point of view?
    Creative Decoration of Sales Area

5)  What do GLB represent from your point of view?
    Appealing Arrangement of Shop Windows

6)  What do GLB represent from your point of view?
    France

7)  What do GLB represent from your point of view?
    French Savoir-vivre

8)  What do GLB represent from your point of view?
    Expertise in French Traditional Cuisine

9)  What do GLB represent from your point of view?
    French Fashion

10) What do GLB represent from your point of view?
    Gourmet Food

11) What do GLB represent from your point of view?
    High-quality Cosmetics

12) What do GLB represent from your point of view?
    Luxury brands

13) What do GLB represent from your point of view?
    Up to date Designer Brands

14) What do GLB represent from your point of view?
    Gourmet specialties

15) What do GLB represent from your point of view?
    Professional Selection of Brands

16) What do GLB represent from your point of view?
    Professional Appearance Towards Customers

17) What do GLB represent from your point of view?
    Are Trendy

18) What do GLB represent from your point of view?
    Are Hip

19) What do GLB represent from your point of view?
    Professional Organization

20) What do GLB represent from your point of view?
    Relaxing Shopping

21) What do GLB represent from your point of view?
    A Great Place to Stroll

22) What do GLB represent from your point of view?
    Intimate Shop Atmosphere

**PFA**

-   RC1 --\> 3,4,5 --\> Decoration

-   RC3 --\> 20,21,22 --\> Atmosphere or Ambiance

-   RC4 --\> 11,12,13 --\> Brand

-   RC5 -\> 1,2 --\> Variety

-   RC2 --\> 10,14 --\> Food or Cuisine

-   RC8 --\> 6-7 --\> Related to France

-   RC6 -\> 17-18 -\>Fashion or Mode

-   RC7 -\> 16-19 -\>Professionalism

**PC**

-   PA1 --\> 3,4,5 --\> Decoration

-   PA3 --\> 20,21,22 --\> Atmosphere or Ambiance

-   PA4 --\> 11,12,13 --\> Brand

-   PA5 -\> 1,2 --\> Variety

-   PA2 --\> 10,14 --\> Food or Cuisine

-   PA7 --\> 6-7 --\> Related to France

-   PA6 -\> 17-18 -\>Fashion or Mode

-   PA8 -\> 16-19 -\>Professionalism

end

# 1- What are the dimensions by which Galeries Lafayette is perceived? Please explain your findings and rational for your final result.

```{r}
model <- "
decoration=~ Im3+Im4+Im5
atmosphere=~ Im20+Im21+Im22
brand=~ Im11+Im12+Im13
variety=~ Im1 + Im2
cuisine=~ Im10+ Im14
france=~ Im6 + Im7
mode=~ Im17 + Im18
professionalism =~ Im16+ Im19"

fit <- cfa(model, image[,-c(8,9,15)], missing="ML")
 summary(fit, fit.measures=TRUE,standardized=TRUE)
```

## Results

We will check the fit measure of our model to determine if it a good model or not.
First global fit measure: From course slide 68 : A low Chi2-value (under consideration of degrees of freedom) indicates a good fit -- ratio Chi2-value/df should be below 5 for samples up to 1000.
In our case we have 553 observations so we are in this case now we calculate: 259.047/ 124 = 2.089089 We are below 5, so result Chi-squared test √

From course slide 69: We need a RMSEA below 0.05 to have a good model.
In our case we have 0.044 and robust RMSEA 0.045 we do have a good model then RMSEA √

Last to check is the CFI Comparative Fit Index, from slide 70 course we need one above 0.95 to have a good model.
In our case we have one of 0.982.
CFI √

We know that our global fit measure a good and our model also.
We then conclude that our factor analysis was good for our model.

```{r}
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE)%>% kable()
```

Wee see that all values are significant.
By looking at the upper and lower case as seen in class it should not contain 0, the case for all execept IM 1 and Im 7 with themselves.
Result are nevertheless very good.

#### Look at coorrelation Matrix

```{r}
std_fit=inspect(fit, "std")
std_fit$psi
```

Looking at correlation Matrix we see all are below 0.7 which is good if not we could issues with discriminant validity, but it is not our case (slide 78)

# 2. Are the mechanism driving satisfaction and affective commitment similar? Are satisfaction and affective commitment mediating the impact of image perceptions on outcomes? If yes for which outcomes?

We need to write the model knowing

images --\> mediators --\> outcome

Images we have our 8 dimensions found previously.

2 mediators --\> Affective Commitment ( with COM_A1 -- COM_A4) Customer Satisfaction ( with SAT_1 -- SAT_3)

2 outcomes --\> Repurchase Intention (with C_REP1-C_REP3 ) CoCreation ( with C_CR1, C.CR3, C.CR4)

Let's write our model:

```{r}
model1 <- "
# measurement model (=~)
decoration=~ Im3+Im4+Im5
atmosphere=~ Im20+Im21+Im22
luxury=~ Im11+Im12+Im13
variety=~ Im1 + Im2
gourmet=~ Im10+ Im14
france=~ Im6 + Im7
mode=~ Im17 + Im18
professionalism =~ Im16+ Im19

  satisfaction =~ SAT_1 + SAT_2 + SAT_3
  commitment =~ COM_A1 + COM_A2 + COM_A3 + COM_A4
  cocreation =~ C_CR1 + C_CR3 + C_CR4
  repurchase =~ C_REP1 + C_REP2 + C_REP3

# Structural model ( ~)
cocreation ~ a * satisfaction + b * commitment
repurchase ~ c * satisfaction + d * commitment 

satisfaction ~ e * professionalism + f * mode + g * france + h * gourmet + i * variety + j * luxury + k * atmosphere + l * decoration
commitment ~ m * professionalism + n * mode + o * france + p * gourmet + q * variety + r * luxury + s * atmosphere + t * decoration

cocreation ~ u * professionalism + v * mode + w * france + x * gourmet + y * variety + z * luxury + aa * atmosphere + bb * decoration
repurchase ~  cc * professionalism + dd * mode + ee * france + ff * gourmet + gg * variety + hh * luxury + ii * atmosphere + jj * decoration


# indirect effect (:=)
# for cocreation: 
  ae:=a*e
  af:=a*f
  ag:=a*g
  ah:=a*h
  ai:=a*i
  aj:=a*j
  ak:=a*k
  al:=a*l
  
  bm:=b*m
  bn:=b*n
  bo:=b*o
  bp:=b*p
  bq:=b*q
  br:=b*r
  bs:=b*s
  bt:=b*t
  
# for repurchase

  ce:=c*e
  cf:=c*f
  cg:=c*g
  ch:=c*h
  ci:=c*i
  cj:=c*j
  ck:=c*k
  cl:=c*l
  
  dm:=d*m
  dn:=d*n
  do:=d*o
  dp:=d*p
  dq:=d*q
  dr:=d*r
  ds:=d*s
  dt:=d*t
  
# Total effects ( := TE)
# for cocreation
TE1C:= u + (a*e) + (b*m)
TE2C:= v + (a*f) + (b*n)
TE3C:= w + (a*g) + (b*o)
TE4C:= x + (a*h) + (b*p)
TE5C:= y + (a*i) + (b*q)
TE6C:= z + (a*j) + (b*r)
TE7C:= aa + (a*k) + (b*s)
TE8C:= bb + (a*l) + (b*t)

# for repurchase 
TE1R:= cc + (c*e) + (d*m)
TE2R:= dd + (c*f) + (d*n)
TE3R:= ee + (c*g) + (d*o)
TE4R:= ff + (c*h) + (d*p)
TE5R:= gg + (c*i) + (d*q)
TE6R:= hh + (c*j) + (d*r)
TE7R:= ii + (c*k) + (d*s)
TE8R:= jj + (c*l) + (d*t)

# total indirect effect
# for cocreation 
TIE1C:=  (a*e) + (b*m)
TIE2C:=  (a*f) + (b*n)
TIE3C:=  (a*g) + (b*o)
TIE4C:=  (a*h) + (b*p)
TIE5C:=  (a*i) + (b*q)
TIE6C:=  (a*j) + (b*r)
TIE7C:=  (a*k) + (b*s)
TIE8C:=  (a*l) + (b*t)

# for repurchase 
TIE1R:=  (c*e) + (d*m)
TIE2R:=  (c*f) + (d*n)
TIE3R:=  (c*g) + (d*o)
TIE4R:=  (c*h) + (d*p)
TIE5R:=  (c*i) + (d*q)
TIE6R:=  (c*j) + (d*r)
TIE7R:=  (c*k) + (d*s)
TIE8R:=  (c*l) + (d*t)

"
fit1<-cfa(model1, data=myData,estimator="MLR", missing="ML")

summary(fit1, fit.measures=TRUE,standardized=TRUE)
```

Before looking anything first see is the model is good by looking at the global fit:

From course slide 68 : A low Chi2-value (under consideration of degrees of freedom) indicates a good fit -- ratio Chi2-value/df should be below 5 for samples up to 1000.
In our case we have 553 observations so we are in this case now we calculate: 632.247/ 399 = 1.584579 ( scaled measure) 700.455/ 399 = 1.755526 ( standard measure) We are below 5, so Chi-squared test good √

From course slide 69: We need a RMSEA below 0.05 to have a good model.
In our case we have RMSEA = 0.037 and Robust RMSE = 0.034 we do have a good model since RMSEA respected √

Last to check is the CFI Comparative Fit Index, from slide 70 course we need one above 0.95 to have a good model.
In our case we have a CFI = 0.974.
CFI √

We know that our global fit measure are good and our model also.

## Plot our results:

```{r}
semPaths(fit1, nCharNodes = 0, style = "lisrel", rotation = 2)
```

# 2.1. Are the mechanism driving the 2 outcome similar?

We see in the regression sections that customer satisfaction (a) has a negative impact on Cocreation (-0.357) meaning a change in one unit increase of cocreation, the value of the response variable decrease of 0.357 units.
We see that the result is significant at a 95% CI (p-value 0.012).
But customer satisfaction (c) has a positive impact on repurchase (0.215) meaning a change in one unit increase of repurchase intention the value of the response variable increase of 0.125 units.
We also see this result is significant at a 95% CI( p-value 0.00).

We already see with this example that the driving in the 2 outcome are different for satisfaction.
<br> For the commitment variables in both case the estimate is significant at 95%CI ( p-value 0.00 in both).
But we see an estimate higher for Cocreation than for repurchase.
Both have a positive impact but at different magnitude.
A change in one unit increase of Cocreation intention the estimated change in the commitment variable is 0.546 units.
Compare to a change in one unit increase of repurchase intention the estimated change in the commitment variable is 0.184 units.

We see a higher increase so we can say with those 2 examples that the mechanisms are different.

# 2.2 Are satisfaction and affective commitment mediating the impact of image perceptions on outcomes? If yes for which outcomes?

First still in the regression part we see that no image has a direct effect on Cocreation or repurchase.
None has a significant p-value.
we can see it in the regression part where <br> cocreation \~ on all the 8 factors <br> repurchase \~ on alp the 8 factors <br>

But we have indirect effect where satisfaction and affective commitment mediating the impact of image perceptions on outcomes.
To check this we look in the defined parameters.
We see the one that are significant: <br> We have ae p-value= 0.036 and std.lv= -0.091 <br> We also have ai with p-value = 0.039 and std.lv= -0.038 <br>

a for satisfaction (in the cocreation regression), e for prfssnlsm, i for variety.
<br> FOR EXAMPLE: This means that satisfction mediates the impact of professionalism on Cocreation.
We have <br> professionalism --\> satisfaction --\> Cocreation <br> is signifciant meaning that if we have more professionalism then the satisfaction of the customer is also higher and as we saw just before the cocreation will be lower (customer satisfaction (a) has a negative impact on Cocreation (-0.357))

For variety it is the same reasoning.
<br>

Now for b ( commitment on cocreation).
we have: bo p-value=0.003 and std.lv= 0.072 <br> bs p-value= 0.000 and std.lv= 0.155 <br>

o for france , s for atmosphere.
<br> FOR EXAMPLE: This means that satisfction mediates the impact of France on Cocreation.
We have <br> France --\> commitment --\> Cocreation <br> is signifciant meaning that if we have more france (object, culture ) then the comitment of the customer is also higher and as we saw just before the cocreation will be higher (customer satisfaction (a) has a negative impact on Cocreation (-0.357))

We have the same reasoning for atmosphere.
<br>

Now for c( satisfaction on repurchase intention ), we have that : ce p-value= 0.003 and std.lv= 0.152 <br> ci p-value 0.027 and std.lv= 0.063 <br> cl p-value 0.049 and std.lv= -0.048 <br>

e for professionalism, i for variety and l for decoration.
We see that both 3 are signifcant meaning that satisfction mediates the impact of professionalism, variety and decoration on repurchase intention.
We have however professionalism and variety ( positive estimate) vs decoration ( negative ) estimate which means that will have contrary impact on repurchase intention.

Now for Now for d(commitment on repurchase intention ), we have that : do p-value= 0.003 and std.lv= 0.068 <br> ds p-value 0.000 and std.lv= 0.145 <br>

o for France and s for atmosphere we see that both 2 are signifcant meaning that commitment mediates the impact of France and amosphere on repurchase intention.
It will both have at the end a positive impact on repurchase intention.

# 3 What is driving the two distinct outcomes? Which image dimensions have the largest total effect on each of them?

Commitment has the biggest effects on Cocreation.
It drives the cocreation otucome.
For Repuchase intention even if the magnitude are different but not relatively different both satisfaction and commitment has impact on the repurchase intention

To see which one has the largest total effect we look at the defined parameter section.
Total effect of TE7R and TE7C are the only significant one.
This is the image of atmosphere.
So atmosphere have the largest total effect on repurchase intention and on commitment 0.254 and 0.256.
which are the biggest value.

For the indirect effect significant we have ( the one from image atmosphere), logical since total effect significant.
<br> we have also TIE3R and TIE3C which are indirect effect of france on cocreation and repurchase intention.
We also haveTIE4R which is gourmet indirect effect on repruchase intwntion that is significant.
<br> The last significant indirect effect with the largest total indirect effect is TIE1R with std.lv of 0.197.
It is the inidrect effect of professionalism on repuchase intention.
